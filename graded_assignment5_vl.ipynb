{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samKhallaghi/02456-deep-learning-with-PyTorch/blob/master/graded_assignment5_vl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying Deep Learning to Earth Observation - Assignment 5**"
      ],
      "metadata": {
        "id": "SmaQLda6ksLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "Work through the assignment 5 notebook, and use this notebook to provide your answers. \n",
        "\n",
        "To submit the assignment, you will need to use GitHub and the existing private repository you already created called `adleoxyz` (xyz is your initials)\n",
        "\n",
        "Once you have completed the assignment:\n",
        "- Commit your notebook from colab to your private GitHub repo\n",
        "- The notebook should be named assignment4_xyz.ipynb, with xyz again replaced by your initials.\n",
        "- There are 50 points in this assignment, with an additional 5 extra credit. "
      ],
      "metadata": {
        "id": "CdWJ1uMOkzku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical questions**"
      ],
      "metadata": {
        "id": "HCbyhUDqlAmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to Q1** \n",
        "\n"
      ],
      "metadata": {
        "id": "2SUTRTwblE5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to Q2** \n",
        "\n",
        "An important feature of RNNs is their ability to handle sequence inputs of varying lengths. Through the feedback mechanism of recurrent neurons, RNN calculates the input at each moment and the state at the previous moment, outputs the state at the current moment, and then transmits it to the next moment, thereby realizing continuous processing of the sequence. Since the state of the recurrent neuron can remember the previous input information, RNN can adaptively process sequence inputs of different lengths, because its state will be adjusted accordingly according to the different lengths of the input, so that while retaining the previous information, it can also be effective to capture long-range dependencies in sequences."
      ],
      "metadata": {
        "id": "ND3Jdir0lBxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 2.5/5 points on optional optional Q2</font>**\n",
        "\n",
        "<p><font color='blue'> Keyword here is weight-sharing. Specifically when we enroll in time the weights for input-to-hidden, hidden-to-hidden and hidden-to-output transitions are shared between the timepoints which makes the length of the sequence irrelevant. Simillar to shared kernel weights in CNNs which makes the variation in the spatial dimension of the image irrelevant.</font> </p>"
      ],
      "metadata": {
        "id": "zbfUFO2dwkz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Continue and complete our pipeline implementation**\n",
        "\n"
      ],
      "metadata": {
        "id": "QkhDBO7WlOtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "csfWTTctkunl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4186fe-c41b-4abc-aaa8-b51d396f7da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbhJLDyAkjZ5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import rasterio\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "cRb1Rvq_lc7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Utillity functions**"
      ],
      "metadata": {
        "id": "W6aYx27Vm44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path, is_label=False, apply_normalization=False, dtype=np.float32, verbose=False):\n",
        "    r\"\"\"\n",
        "    Open data using gdal, read it as an array and normalize it.\n",
        "    Arguments:\n",
        "            data_path (string) -- Full path including filename of the data source we wish to load.\n",
        "            is_label (binary) -- If True then the layer is a ground truth (category index) and if\n",
        "                                set to False the layer is a reflectance band.\n",
        "            apply_normalization (binary) -- If true min/max normalization will be applied on each band.\n",
        "            dtype (np.dtype) -- Data type of the output image chips.\n",
        "            verbose (binary) -- if set to true, print a screen statement on the loaded band.\n",
        "    Returns:\n",
        "            image -- Returns the loaded image as a 32-bit float numpy ndarray with shape (height, width, num_bands).\n",
        "    \"\"\"\n",
        "\n",
        "    # Inform user of the file names being loaded from the Dataset.\n",
        "    if verbose:\n",
        "        print('loading file:{}'.format(data_path))\n",
        "\n",
        "    # open dataset using rasterio library.\n",
        "    with rasterio.open(data_path, \"r\") as src:\n",
        "\n",
        "        if is_label:\n",
        "            if src.count != 1:\n",
        "                raise ValueError(\"Expected Label to have exactly one channel.\")\n",
        "            img = src.read(1)\n",
        "\n",
        "        else:\n",
        "            if apply_normalization:\n",
        "                img = min_max_normalize_image(src.read())\n",
        "                img = img.astype(dtype)\n",
        "            else:\n",
        "                img = src.read()\n",
        "                img = img.astype(dtype)\n",
        "\n",
        "    return img\n",
        "\n",
        "#########################\n",
        "\n",
        "def get_meta_from_bounds(image, overlap=None):\n",
        "    with rasterio.open(image, \"r\") as src:\n",
        "        meta = src.meta\n",
        "    \n",
        "    if overlap:\n",
        "        dst_width = src.width - 2 * overlap\n",
        "        dst_height = src.height - 2 * overlap\n",
        "        window = Window(overlap, overlap, dst_width, dst_height)\n",
        "        win_transform = src.window_transform(window)\n",
        "        meta.update({\n",
        "            'width': dst_width,\n",
        "            'height': dst_height,\n",
        "            'transform': win_transform,\n",
        "            'count': 1,\n",
        "            'dtype': 'int8'\n",
        "        })\n",
        "\n",
        "    return meta\n",
        "\n",
        "#########################\n",
        "\n",
        "def load_params(params_dir, model, freeze_params=None):\n",
        "    \"\"\"Load a set of parameters into a PyTorch model.\n",
        "    Args:\n",
        "        params_dir (str): Path to the \".pth\" or \".pt\" file containing the input parameters.\n",
        "        model (pytorch nn object): Initialized model.\n",
        "        freeze_params (list[int] or None, optional): List of parameter indices to freeze (i.e., set to\n",
        "            require no gradients). Default is None, which means that all parameters are trainable.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the specified file does not exist.\n",
        "\n",
        "    Example:\n",
        "        # Load parameters from file and freeze the first layer\n",
        "        load_params(\"params.pth\", freeze_params=[0])\n",
        "    \"\"\"\n",
        "    # Load input parameters from file\n",
        "    input_params = torch.load(params_dir)\n",
        "\n",
        "    # Filter out parameters that are not in the model    \n",
        "    model_dict = model.state_dict()\n",
        "    if \"module\" in list(input_params.keys())[0]:\n",
        "        input_params_filter = {k[7:]: v.cpu() for k, v in input_params.items() if k[7:] in model_dict}\n",
        "    else:\n",
        "        input_params_filter = {k: v.cpu() for k, v in input_params.items() if k in model_dict}\n",
        "\n",
        "    # Update the model parameters with the input parameters    \n",
        "    model_dict.update(input_params_filter)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # free some layers\n",
        "    if freeze_params is not None:\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            if i in freeze_params:\n",
        "                p.requires_grad = False\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "1aQwho5Mm-ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing steps**"
      ],
      "metadata": {
        "id": "_HgcYTNIloOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code for input normalization\n",
        "\n",
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ],
      "metadata": {
        "id": "SJybOajXl0dZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code for image augmentation techniques\n",
        "\n",
        "def flip_image_and_label(image, label, flip_type):\n",
        "    \"\"\"\n",
        "    Applies horizontal or vertical flip augmentation to an image patch and label\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        flip_type (string) : Based on the direction of flip. Can be either \n",
        "            'hflip' or 'vflip'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the flipped image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if flip_type == 'hflip':\n",
        "        # Apply horizontal flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "        \n",
        "        # Apply horizontal flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 1)\n",
        "        \n",
        "    elif flip_type == 'vflip':\n",
        "        # Apply vertical flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "        \n",
        "        # Apply vertical flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 0)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(\"Flip direction must be 'horizontal' or 'vertical'.\")\n",
        "        \n",
        "    # Return the flipped image patch and label as a tuple\n",
        "    return flipped_image.copy(), flipped_label.copy()\n",
        "\n",
        "\n",
        "def rotate_image_and_label(image, label, angle):\n",
        "    \"\"\"\n",
        "    Applies rotation augmentation to an image patch and label.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        angle (lost of floats) : If the list has exactly two elements they will\n",
        "            be considered the lower and upper bounds for the rotation angle \n",
        "            (in degrees) respectively. If number of elements are bigger than 2, \n",
        "            then one value is chosen randomly as the roatation angle.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the rotated image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(angle, tuple) or isinstance(angle, list):\n",
        "        if len(angle) == 2:\n",
        "            rotation_degree = random.uniform(angle[0], angle[1])\n",
        "        elif len(angle) > 2:\n",
        "            rotation_degree = random.choice(angle)\n",
        "        else:\n",
        "            raise ValueError(\"Parameter degree needs at least two elements.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Rotation bound param for augmentation must be a tuple or list.\"\n",
        "        )\n",
        "    \n",
        "    # Define the center of the image patch\n",
        "    center = tuple(np.array(label.shape)/2.0)\n",
        "\n",
        "    # Define the rotation matrix\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_degree, 1.0)\n",
        "\n",
        "    # Apply rotation augmentation to the image patch\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[:2], \n",
        "                                   flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply rotation augmentation to the label\n",
        "    rotated_label = cv2.warpAffine(label, rotation_matrix, label.shape[:2], \n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Return the rotated image patch and label as a tuple\n",
        "    return rotated_image.copy(), np.rint(rotated_label.copy())"
      ],
      "metadata": {
        "id": "oIkeH2ful8fW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code to get the index of center of each smaller chip\n",
        "\n",
        "def patch_center_index(cropping_ref, patch_size, overlap, usage, \n",
        "                       positive_class_threshold=None, verbose=True):\n",
        "    r\"\"\"\n",
        "    Generate index to divide the scene into small chips.\n",
        "    Each index marks the location of corresponding chip center.\n",
        "    Arguments:\n",
        "        cropping_ref (list) : Reference raster layers, to be used to generate \n",
        "            the index. In our case, it is study area binary mask and label mask.\n",
        "        patch_size (int) : Size of each clipped patches.\n",
        "        overlap (int) : amount of overlap between the extracted chips.\n",
        "        usage (str) : Either 'train', 'val'. Chipping strategy is different for \n",
        "            different usage.\n",
        "        positive_class_threshold (float) : A real value as a threshold for the \n",
        "            proportion of positive class to the total areal of the chip. Used to \n",
        "            decide if the chip should be considered as a positive chip in the \n",
        "            sampling process.\n",
        "    verbose (binary) : If set to True prints on screen the detailed list of \n",
        "            center coordinates of the sampled chips.\n",
        "    Returns:\n",
        "        proportional_patch_index : A list of index recording the center of \n",
        "        patches to extract from the input\n",
        "    \"\"\"\n",
        "\n",
        "    assert usage in [\"train\", \"validation\", \"inference\"]\n",
        "\n",
        "    if usage == \"inference\":\n",
        "        mask = cropping_ref\n",
        "    else:\n",
        "        mask, label = cropping_ref\n",
        "\n",
        "    half_size = patch_size // 2\n",
        "    step_size = patch_size - 2 * overlap\n",
        "\n",
        "    proportional_patch_index = []\n",
        "    non_proportional_patch_index = []\n",
        "    neg_patch_index = []\n",
        "\n",
        "    # Get the index of all the non-zero elements in the mask.\n",
        "    x = np.argwhere(mask)\n",
        "\n",
        "    # First col of x shows the row indices (height) of the mask layer \n",
        "    # (iterate over the y axis or latitude).\n",
        "    x_min = min(x[:, 0]) + half_size\n",
        "    x_max = max(x[:, 0]) - half_size\n",
        "    # Second col of x shows the column indices (width) of the mask layer \n",
        "    # (iterate over the x axis or longitude).\n",
        "    y_min = min(x[:, 1]) + half_size\n",
        "    y_max = max(x[:, 1]) - half_size\n",
        "\n",
        "    # Generate index for the center of each patch considering the proportion of \n",
        "    # each category falling into each patch.\n",
        "    for j in range(y_min, y_max + 1, step_size):\n",
        "\n",
        "        for i in range(x_min, x_max + 1, step_size):\n",
        "\n",
        "            # Split the mask and label layers into patches based on the index of \n",
        "            # the center of the patch\n",
        "            mask_ref = mask[i - half_size: i + half_size, \n",
        "                            j - half_size: j + half_size]\n",
        "            if usage != \"inference\":\n",
        "                label_ref = label[i - half_size: i + half_size, \n",
        "                                  j - half_size: j + half_size]\n",
        "\n",
        "            if (usage == \"train\") and mask_ref.all():\n",
        "\n",
        "                if label_ref.any() != 0:\n",
        "                    pond_ratio = np.sum(label_ref == 1) / label_ref.size\n",
        "                    if pond_ratio >= positive_class_threshold:\n",
        "                        proportional_patch_index.append([i, j])\n",
        "                else:\n",
        "                    neg_patch_index.append([i, j])\n",
        "\n",
        "            if (usage == \"validation\") and (label_ref.any() != 0) and mask_ref.all():\n",
        "                non_proportional_patch_index.append([i, j])\n",
        "            \n",
        "            if (usage == \"inference\") and (mask_ref.any() != 0):\n",
        "                non_proportional_patch_index.append([i, j])\n",
        "\n",
        "    if usage == \"train\":\n",
        "\n",
        "        num_negative_samples = min(\n",
        "            math.ceil(0.2 * len(proportional_patch_index)), 15\n",
        "        )\n",
        "        neg_samples = random.sample(neg_patch_index, num_negative_samples)\n",
        "\n",
        "        proportional_patch_index.extend(neg_samples)\n",
        "\n",
        "    # For test set use the indices generated from mask without \n",
        "    # considering the class proportions.\n",
        "    if usage in [\"validation\", \"inference\"]:\n",
        "        proportional_patch_index = non_proportional_patch_index\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Number of patches:\", len(proportional_patch_index))\n",
        "        print(\"Patched from:\\n{}\".format(proportional_patch_index))\n",
        "\n",
        "    return proportional_patch_index"
      ],
      "metadata": {
        "id": "_9eOqlS75wjT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to modify our custom dataset to handle the \"inference\" phase (also called \"prediction\" or \"test\").\n",
        "\n",
        "The process is very similar to the custom dataset you have used in assignment 3. The prediction scenes are organized in a \"csv\" file. Pixel values are atmospherically corrected to ground reflectance (Values are between 0 and 1) if you want you can standardize it or leave it as is.\n",
        "\n"
      ],
      "metadata": {
        "id": "p5eOcGU63TJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code for Custom dataset -- MUST get modified\n",
        "\n",
        "class AquacultureData(Dataset):\n",
        "    def __init__(self, src_dir, usage, dataset_name=None, apply_normalization=False, \n",
        "                 transform=None, csv_name=None, patch_size=None, overlap=None, catalog_index=None):\n",
        "        r\"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing \n",
        "                              structured folders for image, label\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is string name of the transformation to be used.\n",
        "        \"\"\"\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.csv_name = csv_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "        self.patch_size = patch_size\n",
        "        self.overlap = overlap\n",
        "        \n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\", \"inference\"], \"Usage is not recognized.\"\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            assert self.dataset_name is not None\n",
        "            img_dir = Path(src_dir) / self.dataset_name / self.usage / \"bands\"\n",
        "            img_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(img_dir) for f in \n",
        "                          filenames if f.endswith(\".tif\")]\n",
        "            img_fnames.sort()\n",
        "        \n",
        "            lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "            lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for f in filenames if f.endswith(\".tif\")]\n",
        "            lbl_fnames.sort()\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.lbl_chips = []\n",
        "        \n",
        "            for img_path, lbl_path in tqdm.tqdm(zip(img_fnames, lbl_fnames), total=len(img_fnames)):\n",
        "                img_chip = load_data(img_path, is_label=False, apply_normalization=self.apply_normalization)\n",
        "                img_chip = img_chip.transpose((1, 2, 0))\n",
        "\n",
        "                lbl_chip = load_data(lbl_path, is_label=True)\n",
        "\n",
        "                self.img_chips.append(img_chip)\n",
        "                self.lbl_chips.append(lbl_chip)\n",
        "            \n",
        "            print('--------------{} patches cropped--------------'.format(len(self.img_chips)))\n",
        "        \n",
        "        # This part handles prediction dataset\n",
        "        else:\n",
        "            assert self.csv_name is not None\n",
        "            \n",
        "            ##### Add your code to read the \"csv\" file. (Expected 1 line)\n",
        "            ##### use \"iloc\" and \"catalog_index\" to grab one line of catalog. (Expected 1 line)\n",
        "            df = pd.read_csv(Path(src_dir) /csv_name)\n",
        "            self.catalog = df.iloc[catalog_index]\n",
        "\n",
        "            self.tile = (self.catalog[\"wrs_path\"], self.catalog[\"wrs_row\"])\n",
        "\n",
        "            img_path_ls = [self.catalog[\"img_dir\"]]\n",
        "            mask_path_ls = [self.catalog[\"mask_dir\"]]\n",
        "            \n",
        "            self.meta = get_meta_from_bounds(Path(src_dir) / img_path_ls[0])\n",
        "\n",
        "            half_size = self.patch_size // 2\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.coor = []\n",
        "\n",
        "            for img_path, mask_path in zip(img_path_ls, mask_path_ls):\n",
        "                \n",
        "                ###### Add your code to load the image and assign it to a variable called \"img\". \n",
        "                ###### Use the \"load_data\" function, provided in the utility function. (Expected 1 line)\n",
        "                img = load_data(Path(src_dir)/img_path)\n",
        "                img = img.transpose((1, 2, 0))\n",
        "                \n",
        "                ##### Load your mask again using \"load_data\" function. (Expected 1 line)\n",
        "                mask = load_data(Path(src_dir)/mask_path, is_label=True)\n",
        "                crop_ref = mask\n",
        "\n",
        "                index = patch_center_index(crop_ref, self.patch_size, self.overlap, self.usage)\n",
        "\n",
        "                for i in range(len(index)):\n",
        "                    x = index[i][0]\n",
        "                    y = index[i][1]\n",
        "\n",
        "                    self.img_chips.append(img[x - half_size: x + half_size, y - half_size: y + half_size, :])\n",
        "                    self.coor.append([x, y])\n",
        "\n",
        "            \n",
        "        \n",
        "            print('--------------{} patches cropped--------------'.format(len(self.img_chips)))\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            image_chip = self.img_chips[index]\n",
        "            label_chip = self.lbl_chips[index]\n",
        "\n",
        "            if self.usage == \"train\" and self.transform:\n",
        "                trans_flip_ls = [m for m in self.transform if \"flip\" in m]\n",
        "                if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
        "                    trans_flip = random.sample(trans_flip_ls, 1)[0]\n",
        "                    image_chip, label_chip = flip_image_and_label(image_chip, label_chip, trans_flip)\n",
        "            \n",
        "                if random.randint(0, 1) and \"rotate\" in self.transform:\n",
        "                    img_chip, lbl_chip = rotate_image_and_label(image_chip, label_chip, angle=[0,90])\n",
        "\n",
        "            # Convert numpy arrays to torch tensors.\n",
        "            # Image chips should be: CHW if not transpose to correct order of dimensions.\n",
        "            image_tensor = torch.from_numpy(image_chip.transpose((2, 0, 1))).float()\n",
        "            label_tensor = torch.from_numpy(np.ascontiguousarray(label_chip)).long()\n",
        "\n",
        "            return image_tensor, label_tensor\n",
        "        else:\n",
        "            coor = self.coor[index]\n",
        "            img_chip = self.img_chips[index]\n",
        "            image_tensor = torch.from_numpy(img_chip.transpose((2, 0, 1))).float()\n",
        "\n",
        "            return image_tensor, coor\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.img_chips)"
      ],
      "metadata": {
        "id": "GmGTKqE1mFt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 25/25 points on CA1</font>**"
      ],
      "metadata": {
        "id": "ByCRrrZbw6B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Architecture**"
      ],
      "metadata": {
        "id": "kqw1GSi0nBFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add the Unet model you have designed from assignment 3 or use the existing one.\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    r\"\"\"This module creates a user-defined number of conv+BN+ReLU layers.\n",
        "    Args:\n",
        "        in_channels (int)-- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        kernel_size (int) -- Size of convolution kernel.\n",
        "        stride (int) -- decides how jumpy kernel moves along the spatial \n",
        "            dimensions.\n",
        "        padding (int) -- how much the input should be padded on the borders with \n",
        "            zero.\n",
        "        dilation (int) -- dilation ratio for enlarging the receptive field.\n",
        "        num_conv_layers (int) -- Number of conv+BN+ReLU layers in the block.\n",
        "        drop_rate (float) -- dropout rate at the end of the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                 padding=1, dilation=1, num_conv_layers=2, drop_rate=0):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                            stride=stride, padding=padding, dilation=dilation, \n",
        "                            bias=False),\n",
        "                  nn.BatchNorm2d(out_channels),\n",
        "                  nn.ReLU(inplace=True), ]\n",
        "\n",
        "        if num_conv_layers > 1:\n",
        "            if drop_rate > 0:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels, \n",
        "                              kernel_size=kernel_size, stride=stride, \n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
        "                    nn.Dropout(drop_rate), \n",
        "                ] * (num_conv_layers - 1)\n",
        "            else:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels, \n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), \n",
        "                ] * (num_conv_layers - 1)\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.block(inputs)\n",
        "        return outputs\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class UpconvBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Decoder layer decodes the features along the expansive path.\n",
        "    Args:\n",
        "        in_channels (int) -- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        upmode (str) -- Upsampling type. If \"fixed\" then a linear upsampling with scale factor\n",
        "                        of two will be applied using bi-linear as interpolation method.\n",
        "                        If deconv_1 is chosen then a non-overlapping transposed convolution will\n",
        "                        be applied to upsample the feature maps. If deconv_1 is chosen then an\n",
        "                        overlapping transposed convolution will be applied to upsample the feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, upmode=\"deconv_1\"):\n",
        "        super(UpconvBlock, self).__init__()\n",
        "\n",
        "        if upmode == \"fixed\":\n",
        "            layers = [nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True), ]\n",
        "            layers += [nn.BatchNorm2d(in_channels),\n",
        "                       nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False), ]\n",
        "\n",
        "        elif upmode == \"deconv_1\":\n",
        "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0, dilation=1), ]\n",
        "\n",
        "        elif upmode == \"deconv_2\":\n",
        "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, dilation=1), ]\n",
        "\n",
        "        # Dense Upscaling Convolution\n",
        "        elif upmode == \"DUC\":\n",
        "            up_factor = 2\n",
        "            upsample_dim = (up_factor ** 2) * out_channels\n",
        "            layers = [nn.Conv2d(in_channels, upsample_dim, kernel_size=3, padding=1),\n",
        "                      nn.BatchNorm2d(upsample_dim),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.PixelShuffle(up_factor), ]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Provided upsampling mode is not recognized.\")\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.block(inputs)\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_classes, in_channels, filter_config=None, dropout_rate=0):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if not filter_config:\n",
        "            filter_config = (64, 128, 256, 512, 1024, 2048)\n",
        "\n",
        "        assert len(filter_config) == 6\n",
        "\n",
        "        # Contraction Path\n",
        "        self.encoder_1 = ConvBlock(self.in_channels, filter_config[0], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 64x256x256\n",
        "        self.encoder_2 = ConvBlock(filter_config[0], filter_config[1], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 128x128x128\n",
        "        self.encoder_3 = ConvBlock(filter_config[1], filter_config[2], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 256x64x64\n",
        "        self.encoder_4 = ConvBlock(filter_config[2], filter_config[3], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 512x32x32\n",
        "        self.encoder_5 = ConvBlock(filter_config[3], filter_config[4], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 1024x16x16\n",
        "        self.encoder_6 = ConvBlock(filter_config[4], filter_config[5], num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 2048x8x8\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Expansion Path\n",
        "        self.decoder_1 = UpconvBlock(filter_config[5], filter_config[4], upmode=\"deconv_2\")  # 1024x16x16\n",
        "        self.conv1 = ConvBlock(filter_config[4] * 2, filter_config[4], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_2 = UpconvBlock(filter_config[4], filter_config[3], upmode=\"deconv_2\")  # 512x32x32\n",
        "        self.conv2 = ConvBlock(filter_config[4], filter_config[3], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_3 = UpconvBlock(filter_config[3], filter_config[2], upmode=\"deconv_2\")  # 256x64x64\n",
        "        self.conv3 = ConvBlock(filter_config[3], filter_config[2], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_4 = UpconvBlock(filter_config[2], filter_config[1], upmode=\"deconv_2\")  # 128x128x128\n",
        "        self.conv4 = ConvBlock(filter_config[2], filter_config[1], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_5 = UpconvBlock(filter_config[1], filter_config[0], upmode=\"deconv_2\")  # 64x256x256\n",
        "        self.conv5 = ConvBlock(filter_config[1], filter_config[0], num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.classifier = nn.Conv2d(filter_config[0], n_classes, kernel_size=1, stride=1, padding=0)  # classNumx256x256\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # set_trace()\n",
        "        e1 = self.encoder_1(inputs)  # batch size x 64 x 256 x 256\n",
        "        p1 = self.pool(e1)  # batch size x 64 x 128 x 128\n",
        "\n",
        "        e2 = self.encoder_2(p1)  # batch size x 128 x 128 x 128\n",
        "        p2 = self.pool(e2)  # batch size x 128 x 64 x 64\n",
        "\n",
        "        e3 = self.encoder_3(p2)  # batch size x 256 x 64 x 64\n",
        "        p3 = self.pool(e3)  # batch size x 256 x 32 x 32\n",
        "\n",
        "        e4 = self.encoder_4(p3)  # batch size x 512 x 32 x 32\n",
        "        p4 = self.pool(e4)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        e5 = self.encoder_5(p4)  # batch size x 1024 x 16 x 16\n",
        "        p5 = self.pool(e5)  # batch size x 1024 x 8 x 8\n",
        "\n",
        "        e6 = self.encoder_6(p5)  # batch size x 2048 x 8 x 8\n",
        "\n",
        "        d6 = self.decoder_1(e6)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        \n",
        "        skip1 = torch.cat((e5, d6), dim=1)  # batch size x 2048 x 16 x 16\n",
        "\n",
        "        d6_proper = self.conv1(skip1)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        d5 = self.decoder_2(d6_proper)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        skip2 = torch.cat((e4, d5), dim=1)  # batch size x 1024 x 32 x 32\n",
        "\n",
        "        d5_proper = self.conv2(skip2)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        d4 = self.decoder_3(d5_proper)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        skip3 = torch.cat((e3, d4), dim=1)  # batch size x 512 x 64 x 64\n",
        "\n",
        "        d4_proper = self.conv3(skip3)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        d3 = self.decoder_4(d4_proper)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        skip4 = torch.cat((e2, d3), dim=1)  # batch size x 256 x 128 x 128\n",
        "\n",
        "        d3_proper = self.conv4(skip4)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        d2 = self.decoder_5(d3_proper)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        skip5 = torch.cat((e1, d2), dim=1)  # batch size x 128 x 256 x 256\n",
        "\n",
        "        d2_proper = self.conv5(skip5)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        d1 = self.classifier(d2_proper)  # batch size x classNum x 256 x 256\n",
        "\n",
        "        return d1"
      ],
      "metadata": {
        "id": "onzfydWomxRF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Customized loss function**"
      ],
      "metadata": {
        "id": "yPrJb3q5nTNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loss function\n",
        "\n",
        "class BinaryTverskyFocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Pytorch versiono of tversky focal loss proposed in paper\n",
        "    'A novel focal Tversky loss function and improved Attention U-Net for lesion segmentation'\n",
        "    (https://arxiv.org/abs/1810.07842)\n",
        "    Arguments:\n",
        "        smooth (float): A float number to smooth loss, and avoid NaN error, default: 1\n",
        "        alpha (float): Hyperparameters alpha, paired with (1 - alpha) to shift emphasis to improve recall\n",
        "        gamma (float): Tversky index, default: 1.33\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, smooth=1, alpha=0.7, gamma=1.33):\n",
        "        super(BinaryTverskyFocalLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.alpha = alpha\n",
        "        self.beta = 1 - self.alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # shape: [N, C, *]\n",
        "        assert predict.shape[0] == target.shape[0], \"predict & target batch size do not match\"\n",
        "\n",
        "        # no reduction, same as original paper\n",
        "        predict = predict.contiguous().view(-1)\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        num = (predict * target).sum() + self.smooth\n",
        "        den = (predict * target).sum() + self.alpha * ((1 - predict) * target).sum() \\\n",
        "              + self.beta * (predict * (1 - target)).sum() + self.smooth\n",
        "        loss = torch.pow(1 - num / den, 1 / self.gamma)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class TverskyFocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Tversky focal loss\n",
        "    Arguments:\n",
        "        weight (torch.tensor): Weight array of shape [num_classes,]\n",
        "        ignore_index (int): Class index to ignore\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor with the same shape as predict.\n",
        "        other args pass to BinaryTverskyFocalLoss\n",
        "    Returns:\n",
        "        same as BinaryTverskyFocalLoss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, ignore_index=-100, **kwargs):\n",
        "        super(TverskyFocalLoss, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        nclass = predict.shape[1]\n",
        "        if predict.shape == target.shape:\n",
        "            pass\n",
        "        elif len(predict.shape) == 4:\n",
        "            target = F.one_hot(target, num_classes=nclass).permute(0, 3, 1, 2).contiguous()\n",
        "        else:\n",
        "            assert 'predict shape not applicable'\n",
        "\n",
        "        tversky = BinaryTverskyFocalLoss(**self.kwargs)\n",
        "        total_loss = 0\n",
        "        weight = torch.Tensor([1. / nclass] * nclass).cuda() if self.weight is None else self.weight\n",
        "        predict = F.softmax(predict, dim=1)\n",
        "\n",
        "        for i in range(nclass):\n",
        "            if i != self.ignore_index:\n",
        "                tversky_loss = tversky(predict[:, i], target[:, i])\n",
        "                assert weight.shape[0] == nclass, \\\n",
        "                    'Expect weight shape [{}], get[{}]'.format(nclass, weight.shape[0])\n",
        "                tversky_loss *= weight[i]\n",
        "                total_loss += tversky_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "class BalancedTverskyFocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Tversky focal loss weighted by inverse of label frequency\n",
        "    Arguments:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
        "        other args pass to BinaryTverskyFocalLoss\n",
        "    Returns:\n",
        "        same as TverskyFocalLoss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ignore_index=-100, **kwargs):\n",
        "        super(BalancedTverskyFocalLoss, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
        "        # calculate weight for only valid indices\n",
        "        unique_counts = unique_counts[unique != self.ignore_index]\n",
        "        unique = unique[unique != self.ignore_index]\n",
        "        ratio = unique_counts.float() / torch.numel(target)\n",
        "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
        "\n",
        "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
        "        for i in range(len(unique)):\n",
        "            lossWeight[unique[i]] = weight[i]\n",
        "\n",
        "        # loss\n",
        "        loss = TverskyFocalLoss(weight=lossWeight, ignore_index=self.ignore_index, **self.kwargs)\n",
        "\n",
        "        return loss(predict, target)\n",
        "\n",
        "\n",
        "class TverskyFocalCELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combination of tversky focal loss and cross entropy loss though summation\n",
        "    Arguments:\n",
        "        loss_weight (tensor): a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n",
        "        tversky_weight (float): Weight on tversky focal loss for the summation, while weight on cross entropy loss\n",
        "                                is (1 - tversky_weight)\n",
        "        tversky_smooth (float): A float number to smooth tversky focal loss, and avoid NaN error, default: 1\n",
        "        tversky_alpha (float):\n",
        "        tversky_gamma (float):\n",
        "        ignore_index (int): Class index to ignore\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss_weight=None, tversky_weight=0.5, tversky_smooth=1, tversky_alpha=0.7,\n",
        "                 tversky_gamma=0.9, ignore_index=-100):\n",
        "        super(TverskyFocalCELoss, self).__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "        self.tversky_weight = tversky_weight\n",
        "        self.tversky_smooth = tversky_smooth\n",
        "        self.tversky_alpha = tversky_alpha\n",
        "        self.tversky_gamma = tversky_gamma\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \"predict & target batch size do not match\"\n",
        "\n",
        "        tversky = TverskyFocalLoss(weight=self.loss_weight, ignore_index=self.ignore_index, smooth=self.tversky_smooth,\n",
        "                                   alpha=self.tversky_alpha, gamma=self.tversky_gamma)\n",
        "        ce = nn.CrossEntropyLoss(weight=self.loss_weight, ignore_index=self.ignore_index)\n",
        "        loss = self.tversky_weight * tversky(predict, target) + (1 - self.tversky_weight) * ce(predict, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class BalancedTverskyFocalCELoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Combination of tversky focal loss and cross entropy loss weighted by inverse of label frequency\n",
        "    Arguments:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
        "        other args pass to DiceCELoss, excluding loss_weight\n",
        "    Returns:\n",
        "        Same as TverskyFocalCELoss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ignore_index=-100, **kwargs):\n",
        "        super(BalancedTverskyFocalCELoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
        "        # calculate weight for only valid indices\n",
        "        unique_counts = unique_counts[unique != self.ignore_index]\n",
        "        unique = unique[unique != self.ignore_index]\n",
        "        ratio = unique_counts.float() / torch.numel(target)\n",
        "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
        "\n",
        "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
        "        for i in range(len(unique)):\n",
        "            lossWeight[unique[i]] = weight[i]\n",
        "\n",
        "        loss = TverskyFocalCELoss(loss_weight=lossWeight, **self.kwargs)\n",
        "\n",
        "        return loss(predict, target)"
      ],
      "metadata": {
        "id": "pltcixbxnYIz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model fitting (training + validation)**"
      ],
      "metadata": {
        "id": "NvdpwrzQky0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions to train the model using the training set and do validation on the validation dataset.\n",
        "\n",
        "def train(trainData, model, optimizer, criterion, device, train_loss=[]):\n",
        "    \"\"\"\n",
        "        Train the model using provided training dataset.\n",
        "        Params:\n",
        "            trainData (DataLoader object) -- Batches of image chips from PyTorch custom dataset (AquacultureData).\n",
        "            model -- Choice of segmentation model.\n",
        "            optimizer -- Chosen optimization algorithm to update model parameters.\n",
        "            criterion -- Chosen function to calculate loss over training samples.\n",
        "            gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "            train_loss (empty list, optional) -- ???????????????????????????\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Mini batch iteration\n",
        "    train_epoch_loss = 0\n",
        "    train_batches = len(trainData)\n",
        "\n",
        "    for img_chips, labels in trainData:\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(train_epoch_loss / train_batches)\n",
        "    print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))\n",
        "\n",
        "##########################################################\n",
        "\n",
        "def validate(valData, model, criterion, device, val_loss=[]):\n",
        "    \"\"\"\n",
        "        Evaluate the model on separate Landsat scenes.\n",
        "        Params:\n",
        "            valData (DataLoader object) -- Batches of image chips from PyTorch custom dataset(AquacultureData)\n",
        "            model -- Choice of segmentation Model.\n",
        "            criterion -- Chosen function to calculate loss over validation samples.\n",
        "            buffer: Buffer added to the targeted grid when creating dataset. This allows loss to calculate\n",
        "                at non-buffered region.\n",
        "            gpu (binary,optional): Decide whether to use GPU, default is True\n",
        "            valLoss (empty list): To record average loss for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # mini batch iteration\n",
        "    eval_epoch_loss = 0\n",
        "\n",
        "    for img_chips, labels in valData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        label = Variable(labels, requires_grad=False)\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        eval_epoch_loss += loss.item()\n",
        "\n",
        "    print('validation loss: {}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "    if val_loss != None:\n",
        "        val_loss.append(float(eval_epoch_loss / len(valData)))\n",
        "\n",
        "##########################################################\n",
        "\n",
        "def epochIterater(trainData, valData, model, criterion, WorkingFolder, initial_lr, num_epochs):\n",
        "    r\"\"\"\n",
        "    Epoch iteration for train and evaluation.\n",
        "    \n",
        "    Arguments:\n",
        "    trainData (dataloader object): Batch grouped data to train the model.\n",
        "    evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "    model (pytorch.nn.module object): initialized model.\n",
        "    initial_lr(float): The initial learning rate.\n",
        "    num_epochs (int): User-defined number of epochs to run the model.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cuda\":\n",
        "        print('----------GPU available----------')\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print('----------No GPU available, using CPU instead----------')\n",
        "        model = model\n",
        "    \n",
        "    writer = SummaryWriter(WorkingFolder)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=initial_lr,\n",
        "                           betas=(0.9, 0.999),\n",
        "                           eps=1e-08,\n",
        "                           weight_decay=5e-4,\n",
        "                           amsgrad=False)\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                          step_size=3,\n",
        "                                          gamma=0.90)\n",
        "    \n",
        "    for t in range(num_epochs):\n",
        "        print(\"Epoch [{}/{}]\".format(t + 1, num_epochs))\n",
        "        start_epoch = datetime.now()\n",
        "\n",
        "        train(trainData, model, optimizer, criterion, device, train_loss=train_loss)\n",
        "        validate(valData, model, criterion, device, val_loss=val_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(\"LR: {}\".format(scheduler.get_last_lr()))\n",
        "\n",
        "        writer.add_scalars(\"Loss\", \n",
        "                           {\"train loss\": train_loss[t],\n",
        "                            \"validation loss\": val_loss[t]},\n",
        "                           t + 1)\n",
        "    \n",
        "    writer.close()\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"--------------- Training finished in {} ---------------\".format(duration_format))"
      ],
      "metadata": {
        "id": "fAIWoMqvk-3G",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation and accuracy metrics**"
      ],
      "metadata": {
        "id": "6vPJa4OwlcU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Metrics and evaluation functions\n",
        "\n",
        "\n",
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Pixel_Accuracy_Class(self):\n",
        "        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "        Acc = np.nanmean(Acc)\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        freq = np.sum(self.confusion_matrix, axis=1) / np.sum(self.confusion_matrix)\n",
        "        iu = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n",
        "\n",
        "#########################\n",
        "\n",
        "def do_accuracy_evaluation(model, dataloader, num_classes):\n",
        "    evaluator = Evaluator(num_classes)\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # add batch to evaluator\n",
        "            evaluator.add_batch(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    pixel_accuracy = evaluator.Pixel_Accuracy()\n",
        "    mean_accuracy = evaluator.Pixel_Accuracy_Class()\n",
        "    mean_IoU = evaluator.Mean_Intersection_over_Union()\n",
        "    frequency_weighted_IoU = evaluator.Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "    return pixel_accuracy, mean_accuracy, mean_IoU, frequency_weighted_IoU"
      ],
      "metadata": {
        "id": "QQz-ts_gleRE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction**"
      ],
      "metadata": {
        "id": "buycO80Vo-L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete the code for performing prediction\n",
        "\n",
        "\n",
        "def do_prediction(testData, model, overlap, device, save_dir):\n",
        "    r\"\"\"\n",
        "    Use train model to predict on unseen data.\n",
        "    Arguments:\n",
        "            testData (custom iterator) -- Batches of image chips from PyTorch \n",
        "                                          custom dataset.\n",
        "            model (ordered Dict) -- trained model.\n",
        "            overlap (int) -- amount of overlap between prediction chips.\n",
        "            device (str) -- Either \"cpu\" or \"cuda\".\n",
        "            save_dir (str) -- Directory to save the prediction output.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create directories to save the predicted output\n",
        "    save_dir_hard = Path(save_dir) / \"HardScore\"\n",
        "    save_dir_soft = Path(save_dir) / \"SoftScore\"\n",
        "\n",
        "    os.makedirs(save_dir_hard, exist_ok=True)\n",
        "    os.makedirs(save_dir_soft, exist_ok=True)\n",
        "\n",
        "    # Start inference on test data\n",
        "    print(\"--------------------- Start Inference(Test) ---------------------\")\n",
        "    start = datetime.now()\n",
        "\n",
        "    # Get the test data, metadata, and tile information\n",
        "    # add your code here\n",
        "    testData, meta, tile = testData\n",
        "\n",
        "    # Define the output file names and metadata for the hard and soft scores\n",
        "    name_prob = \"prob_c{}_r{}\".format(tile[0], tile[1])\n",
        "    name_crisp = \"crisp_c{}_r{}.rst\".format(tile[0], tile[1])\n",
        "\n",
        "    meta_hard = meta.copy()\n",
        "    meta_hard.update({\n",
        "        \"dtype\": \"uint8\",\n",
        "        \"count\": 1,\n",
        "    })\n",
        "\n",
        "    meta_soft = meta.copy()\n",
        "    meta_soft.update({\n",
        "        \"dtype\": \"float32\",\n",
        "        \"count\": 1,\n",
        "    })\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    ##### Add your code to put the model in evaluation mode. (1 line)\n",
        "    model.eval()\n",
        "    \n",
        "    canvas_score_ls = []\n",
        "    \n",
        "    ##### Create a canvas (call it \"h_canvas\") with the same height, width and datatype \n",
        "    ##### from \"meta_hard\" to hold the score values and initialize it to zeros.\n",
        "    ##### Add your code here. (Expected 1 line)\n",
        "    h_canvas = np.zeros((1, meta_hard['height'], meta_hard['width']), dtype=meta_hard['dtype'])\n",
        "\n",
        "    # Loop over batches of image chips and indices.\n",
        "    for img_chips, index_batch in testData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        img = img_chips.to(device) # size: B X in_channels X W X H\n",
        "\n",
        "        ##### Forward pass through the model to get the predictions and assign it to a variable called \"pred\".\n",
        "        ##### Add your code here. (Expected 1 line)\n",
        "        pred = model(img)\n",
        "        ##### Normalize the model output using \"softmax\" And assign it to a \n",
        "        ##### variable called \"pred_prob\".\n",
        "        ##### Add your code here (Expected 1 line)\n",
        "        pred_prob = F.softmax(pred, dim=1)\n",
        "        # Get the dimensions of the prediction\n",
        "        batch, n_class, height, width = pred_prob.size()\n",
        "\n",
        "        # Calculate the score width and score height based on the overlap parameter\n",
        "        score_width = (width // 2) - overlap\n",
        "        score_height = (height // 2) - overlap\n",
        "\n",
        "        # Loop over the batch and assign the predicted scores to the canvas\n",
        "        for i in range(batch):\n",
        "\n",
        "            # creating a new tuple index containing the coordinates which makes it \n",
        "            # easier to index into the \"h_canvas\" and arrays in the \"canvas_score_ls\" later on in the code.\n",
        "            index = (index_batch[0][i], index_batch[1][i])\n",
        "\n",
        "            # Get the hard scores by taking the argmax of the prediction\n",
        "            prediction_hard = pred_prob.max(dim=1)[1][:, overlap:-overlap, overlap:-overlap].cpu().numpy()[i, :, :]\n",
        "            \n",
        "            # add the batch dimension to the \"prediction_hard\" array and convert its data types.\n",
        "            prediction_hard = np.expand_dims(prediction_hard, axis=0).astype(meta_hard[\"dtype\"])\n",
        "\n",
        "            # The \"prediction_hard\" values are assigned to a slice of \"h_canvas\", \n",
        "            # effectively updating the pixels in the original image corresponding\n",
        "            # to the current image chip in the batch with the predicted values for that chip.\n",
        "            ##### Add your code here. (Expected 1 line)\n",
        "            h_canvas[:, index[0] - score_width: index[0] + score_width,\n",
        "                    index[1] - score_height: index[1] + score_height] = prediction_hard\n",
        "\n",
        "            for n in range(1, n_class):\n",
        "                # Extract probability map for class n from predicted probabilities tensor\n",
        "                prediction_soft = pred_prob[:, n, :, :].data[i][overlap:-overlap, overlap:-overlap].cpu().numpy() * 100\n",
        "                # Add an extra dimension to the probability map to match the expected shape\n",
        "                prediction_soft = np.expand_dims(prediction_soft, axis=0).astype(meta_soft[\"dtype\"])\n",
        "\n",
        "                try:\n",
        "                    # Update existing canvas for class n with the new probability map\n",
        "                    canvas_score_ls[n][:, index[0] - score_width : index[0] + score_width,\n",
        "                    index[1] - score_height: index[1] + score_height] = prediction_soft\n",
        "                except:\n",
        "                    # Create a new canvas for class n and initialize it with zeros\n",
        "                    canvas_score_single = np.zeros((1, meta_soft['height'],\n",
        "                                                    meta_soft['width']), dtype=meta_soft['dtype'])\n",
        "\n",
        "                    # Update the new canvas with the new probability map slice by slice\n",
        "                    canvas_score_single[:, index[0] - score_width: index[0] + score_width,\n",
        "                    index[1] - score_height: index[1] + score_height] = prediction_soft\n",
        "                    \n",
        "                    # Add the new canvas to the list of canvases for all classes\n",
        "                    canvas_score_ls.append(canvas_score_single)\n",
        "    \n",
        "    # write the hard classification results to an output raster.\n",
        "    ##### Use \"save_dir_hard\", \"name_crisp\" and \"meta_hard\".\n",
        "    ##### Add your code here. (Expected 2 line)\n",
        "    with rasterio.open(os.path.join(save_dir_hard, name_crisp), 'w', **meta_hard) as dst:\n",
        "      dst.write(h_canvas)\n",
        "    \n",
        "    # loop through each class (excluding the background class) and creates a \n",
        "    # new raster file for each class.\n",
        "    ##### Add your code here. (Expected 4 line)\n",
        "    ##### hint: use this code to get a proper name for the prediction output \n",
        "    ##### for each class: name_prob_updated = f\"{name_prob}_Cat_{n}.tif\" \n",
        "    for n in range(1, n_class):\n",
        "      canvas_score = canvas_score_ls[n]\n",
        "\n",
        "      name_prob_updated = f\"{name_prob}_Cat_{n}.tif\"\n",
        "\n",
        "      with rasterio.open(os.path.join(save_dir_soft, name_prob_updated), 'w', **meta_soft) as dst:\n",
        "        dst.write(canvas_score)\n",
        "    \n",
        "    duration_in_sec = (datetime.now() - start).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"---------------- Inference finished in {} seconds ----------------\".format(duration_format))"
      ],
      "metadata": {
        "id": "o5dyyiGNpEEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Running through the pipeline**"
      ],
      "metadata": {
        "id": "pwO-57HOoyWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps to train the model from scratch**"
      ],
      "metadata": {
        "id": "ZXVHjVeixPk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/GEOG315/A5_resources\"\n",
        "dataset_name = \"Global\"\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "criterion = \"BalancedTverskyFocalCELoss()\"\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/GEOG315/Lab5/test_a5\"\n",
        "initial_lr = 0.15\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "mMyd2JGWo8sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a 'train_dataset' object using the 'AquacultureData' class.\n",
        "train_dataset = AquacultureData(src_dir, \n",
        "                                usage=\"train\", \n",
        "                                dataset_name=dataset_name, \n",
        "                                apply_normalization=False, \n",
        "                                transform=transform)"
      ],
      "metadata": {
        "id": "HPbuoeoD6gXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6a1283-ba7c-4f30-e533-b02f414b4c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1188/1188 [02:13<00:00,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------1188 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Pytorch data loader called 'train_loader' that loads data from the \n",
        "# 'train_dataset' dataset, split it into batches, convert is to tensor and move \n",
        "# the data to GPU if available.\n",
        " \n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16, \n",
        "                          shuffle = True)"
      ],
      "metadata": {
        "id": "oNzi0SON6kRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a 'validation_dataset' object using the 'AquacultureData' class.\n",
        "validation_dataset = AquacultureData(src_dir, \n",
        "                                     usage=\"validation\", \n",
        "                                     dataset_name=dataset_name, \n",
        "                                     apply_normalization=False)"
      ],
      "metadata": {
        "id": "vn2UNb4J6snk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f95c6c-ed96-4c26-f65b-d396a957d21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 239/239 [00:21<00:00, 10.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------239 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Pytorch data loader for the 'validation_dataset'.\n",
        "val_loader = DataLoader(validation_dataset, \n",
        "                        batch_size = 1, \n",
        "                        shuffle = False)"
      ],
      "metadata": {
        "id": "v5AHVjVt6vyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model\n",
        "model = Unet(n_classes, \n",
        "             in_channels, \n",
        "             filter_config, \n",
        "             dropout_rate)"
      ],
      "metadata": {
        "id": "OSdkVZtV677s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model fitting (I will use the pretrained one)\n",
        "epochIterater(train_loader, \n",
        "              val_loader, \n",
        "              model, \n",
        "              criterion, \n",
        "              WorkingFolder, \n",
        "              initial_lr, \n",
        "              epochs)"
      ],
      "metadata": {
        "id": "qhyDVe2E69RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57fd785-4906-4773-b162-f2134d3fe7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------GPU available----------\n",
            "Epoch [1/50]\n",
            "Training loss: 0.3872\n",
            "validation loss: 0.7084587193308515\n",
            "LR: [0.15]\n",
            "Epoch [2/50]\n",
            "Training loss: 0.3688\n",
            "validation loss: 0.6237432850704033\n",
            "LR: [0.15]\n",
            "Epoch [3/50]\n",
            "Training loss: 0.3600\n",
            "validation loss: 10.573839703239656\n",
            "LR: [0.135]\n",
            "Epoch [4/50]\n",
            "Training loss: 0.3721\n",
            "validation loss: 558.132881834916\n",
            "LR: [0.135]\n",
            "Epoch [5/50]\n",
            "Training loss: 0.3678\n",
            "validation loss: 1.733028933842312\n",
            "LR: [0.135]\n",
            "Epoch [6/50]\n",
            "Training loss: 0.3473\n",
            "validation loss: 0.7237201066196713\n",
            "LR: [0.12150000000000001]\n",
            "Epoch [7/50]\n",
            "Training loss: 0.3293\n",
            "validation loss: 0.5577019471129613\n",
            "LR: [0.12150000000000001]\n",
            "Epoch [8/50]\n",
            "Training loss: 0.3595\n",
            "validation loss: 1.2040477537211016\n",
            "LR: [0.12150000000000001]\n",
            "Epoch [9/50]\n",
            "Training loss: 0.3333\n",
            "validation loss: 0.43208803697859394\n",
            "LR: [0.10935000000000002]\n",
            "Epoch [10/50]\n",
            "Training loss: 0.3393\n",
            "validation loss: 0.5034537064106395\n",
            "LR: [0.10935000000000002]\n",
            "Epoch [11/50]\n",
            "Training loss: 0.3306\n",
            "validation loss: 0.6069216352875761\n",
            "LR: [0.10935000000000002]\n",
            "Epoch [12/50]\n",
            "Training loss: 0.3535\n",
            "validation loss: 1.8861842265188944\n",
            "LR: [0.09841500000000002]\n",
            "Epoch [13/50]\n",
            "Training loss: 0.3772\n",
            "validation loss: 0.5680808701151085\n",
            "LR: [0.09841500000000002]\n",
            "Epoch [14/50]\n",
            "Training loss: 0.3569\n",
            "validation loss: 0.5958449290636693\n",
            "LR: [0.09841500000000002]\n",
            "Epoch [15/50]\n",
            "Training loss: 0.3389\n",
            "validation loss: 0.5396206888943038\n",
            "LR: [0.08857350000000001]\n",
            "Epoch [16/50]\n",
            "Training loss: 0.3392\n",
            "validation loss: 0.8550733085955536\n",
            "LR: [0.08857350000000001]\n",
            "Epoch [17/50]\n",
            "Training loss: 0.3391\n",
            "validation loss: 0.5300032601820375\n",
            "LR: [0.08857350000000001]\n",
            "Epoch [18/50]\n",
            "Training loss: 0.3383\n",
            "validation loss: 0.35773670461387314\n",
            "LR: [0.07971615000000001]\n",
            "Epoch [19/50]\n",
            "Training loss: 0.3500\n",
            "validation loss: 0.8031251486624634\n",
            "LR: [0.07971615000000001]\n",
            "Epoch [20/50]\n",
            "Training loss: 0.3388\n",
            "validation loss: 0.49088274584654484\n",
            "LR: [0.07971615000000001]\n",
            "Epoch [21/50]\n",
            "Training loss: 0.3394\n",
            "validation loss: 0.36603412880059566\n",
            "LR: [0.07174453500000001]\n",
            "Epoch [22/50]\n",
            "Training loss: 0.3301\n",
            "validation loss: 0.5150524604769431\n",
            "LR: [0.07174453500000001]\n",
            "Epoch [23/50]\n",
            "Training loss: 0.3222\n",
            "validation loss: 0.3243842592236886\n",
            "LR: [0.07174453500000001]\n",
            "Epoch [24/50]\n",
            "Training loss: 0.3414\n",
            "validation loss: 0.559822551750239\n",
            "LR: [0.06457008150000002]\n",
            "Epoch [25/50]\n",
            "Training loss: 0.3129\n",
            "validation loss: 0.32838948934522133\n",
            "LR: [0.06457008150000002]\n",
            "Epoch [26/50]\n",
            "Training loss: 0.3225\n",
            "validation loss: 0.35193858691588603\n",
            "LR: [0.06457008150000002]\n",
            "Epoch [27/50]\n",
            "Training loss: 0.3137\n",
            "validation loss: 0.35575427027676393\n",
            "LR: [0.058113073350000016]\n",
            "Epoch [28/50]\n",
            "Training loss: 0.3249\n",
            "validation loss: 0.48146727858851646\n",
            "LR: [0.058113073350000016]\n",
            "Epoch [29/50]\n",
            "Training loss: 0.3112\n",
            "validation loss: 0.5189093430546038\n",
            "LR: [0.058113073350000016]\n",
            "Epoch [30/50]\n",
            "Training loss: 0.3094\n",
            "validation loss: 1.2024036680804138\n",
            "LR: [0.052301766015000015]\n",
            "Epoch [31/50]\n",
            "Training loss: 0.3664\n",
            "validation loss: 0.4974404160697091\n",
            "LR: [0.052301766015000015]\n",
            "Epoch [32/50]\n",
            "Training loss: 0.3196\n",
            "validation loss: 0.3413821219400382\n",
            "LR: [0.052301766015000015]\n",
            "Epoch [33/50]\n",
            "Training loss: 0.3084\n",
            "validation loss: 0.45261030457134527\n",
            "LR: [0.047071589413500016]\n",
            "Epoch [34/50]\n",
            "Training loss: 0.3093\n",
            "validation loss: 0.3971809995473678\n",
            "LR: [0.047071589413500016]\n",
            "Epoch [35/50]\n",
            "Training loss: 0.2968\n",
            "validation loss: 0.5407148583102426\n",
            "LR: [0.047071589413500016]\n",
            "Epoch [36/50]\n",
            "Training loss: 0.3016\n",
            "validation loss: 0.35017141350642406\n",
            "LR: [0.042364430472150015]\n",
            "Epoch [37/50]\n",
            "Training loss: 0.3159\n",
            "validation loss: 0.5795285465452961\n",
            "LR: [0.042364430472150015]\n",
            "Epoch [38/50]\n",
            "Training loss: 0.3086\n",
            "validation loss: 0.3426515351217162\n",
            "LR: [0.042364430472150015]\n",
            "Epoch [39/50]\n",
            "Training loss: 0.3122\n",
            "validation loss: 0.5294952996729807\n",
            "LR: [0.03812798742493501]\n",
            "Epoch [40/50]\n",
            "Training loss: 0.2964\n",
            "validation loss: 1.3136818860365258\n",
            "LR: [0.03812798742493501]\n",
            "Epoch [41/50]\n",
            "Training loss: 0.2891\n",
            "validation loss: 0.39619009037770986\n",
            "LR: [0.03812798742493501]\n",
            "Epoch [42/50]\n",
            "Training loss: 0.2894\n",
            "validation loss: 0.5097110483436904\n",
            "LR: [0.03431518868244151]\n",
            "Epoch [43/50]\n",
            "Training loss: 0.2916\n",
            "validation loss: 0.44595338214017355\n",
            "LR: [0.03431518868244151]\n",
            "Epoch [44/50]\n",
            "Training loss: 0.2792\n",
            "validation loss: 0.30034676365647855\n",
            "LR: [0.03431518868244151]\n",
            "Epoch [45/50]\n",
            "Training loss: 0.3204\n",
            "validation loss: 0.5413388411619673\n",
            "LR: [0.030883669814197358]\n",
            "Epoch [46/50]\n",
            "Training loss: 0.3035\n",
            "validation loss: 0.6035021610215119\n",
            "LR: [0.030883669814197358]\n",
            "Epoch [47/50]\n",
            "Training loss: 0.3074\n",
            "validation loss: 0.5009877290810502\n",
            "LR: [0.030883669814197358]\n",
            "Epoch [48/50]\n",
            "Training loss: 0.3001\n",
            "validation loss: 0.35174212657756887\n",
            "LR: [0.027795302832777622]\n",
            "Epoch [49/50]\n",
            "Training loss: 0.2838\n",
            "validation loss: 0.3351733022716255\n",
            "LR: [0.027795302832777622]\n",
            "Epoch [50/50]\n",
            "Training loss: 0.3015\n",
            "validation loss: 0.5851014601885025\n",
            "LR: [0.027795302832777622]\n",
            "--------------- Training finished in 0:00:33 ---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model parameters.\n",
        "torch.save(model.state_dict(), \n",
        "           os.path.join(Path(WorkingFolder), \"trained_unet_final_state.pth\"))"
      ],
      "metadata": {
        "id": "kevXvC3dx0HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = do_accuracy_evaluation(model.cuda(), val_loader, 2)"
      ],
      "metadata": {
        "id": "1cbZoaWj7dG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Fine-tuning the model on a a new dataset**"
      ],
      "metadata": {
        "id": "cUQvE6Eux5yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/GEOG315/A5_resources\"\n",
        "dataset_name = \"Fine_tune_dataset\"\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.15\n",
        "\n",
        "criterion = \"BalancedTverskyFocalCELoss()\"\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/GEOG315/Lab5/test_a5_fineT\"\n",
        "initial_lr = 0.01\n",
        "epochs = 25"
      ],
      "metadata": {
        "id": "k5etJBDkx6vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AquacultureData(src_dir, \n",
        "                                usage=\"train\", \n",
        "                                dataset_name=dataset_name, \n",
        "                                apply_normalization=False, \n",
        "                                transform=transform)"
      ],
      "metadata": {
        "id": "KUxy3PsyyEOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fd911f-ecee-4dd8-9117-02fb31cd5d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 758/758 [01:23<00:00,  9.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------758 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16, \n",
        "                          shuffle = True)"
      ],
      "metadata": {
        "id": "hJX30X0SyHKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = AquacultureData(src_dir, \n",
        "                                     usage=\"validation\", \n",
        "                                     dataset_name=dataset_name, \n",
        "                                     apply_normalization=False)"
      ],
      "metadata": {
        "id": "jixCKA-tyHVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bec138d-a4b6-4a43-b2cc-8216b128b10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 122/122 [00:11<00:00, 10.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------122 patches cropped--------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(validation_dataset, \n",
        "                        batch_size = 1, \n",
        "                        shuffle = False)"
      ],
      "metadata": {
        "id": "PeaCsX8zyHef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(n_classes, \n",
        "             in_channels, \n",
        "             filter_config, \n",
        "             dropout_rate)"
      ],
      "metadata": {
        "id": "c1EGnuBLyUeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the named trainable parameters and their indices\n",
        "for i, (name, param) in enumerate(model.named_parameters()):\n",
        "    if param.requires_grad:\n",
        "        print(i, name)"
      ],
      "metadata": {
        "id": "N6WQ12dpKQfd",
        "outputId": "3712bb75-0f0c-469c-d7eb-57b5e8f02381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 encoder_1.block.0.weight\n",
            "1 encoder_1.block.1.weight\n",
            "2 encoder_1.block.1.bias\n",
            "3 encoder_1.block.3.weight\n",
            "4 encoder_1.block.4.weight\n",
            "5 encoder_1.block.4.bias\n",
            "6 encoder_2.block.0.weight\n",
            "7 encoder_2.block.1.weight\n",
            "8 encoder_2.block.1.bias\n",
            "9 encoder_2.block.3.weight\n",
            "10 encoder_2.block.4.weight\n",
            "11 encoder_2.block.4.bias\n",
            "12 encoder_3.block.0.weight\n",
            "13 encoder_3.block.1.weight\n",
            "14 encoder_3.block.1.bias\n",
            "15 encoder_3.block.3.weight\n",
            "16 encoder_3.block.4.weight\n",
            "17 encoder_3.block.4.bias\n",
            "18 encoder_4.block.0.weight\n",
            "19 encoder_4.block.1.weight\n",
            "20 encoder_4.block.1.bias\n",
            "21 encoder_4.block.3.weight\n",
            "22 encoder_4.block.4.weight\n",
            "23 encoder_4.block.4.bias\n",
            "24 encoder_5.block.0.weight\n",
            "25 encoder_5.block.1.weight\n",
            "26 encoder_5.block.1.bias\n",
            "27 encoder_5.block.3.weight\n",
            "28 encoder_5.block.4.weight\n",
            "29 encoder_5.block.4.bias\n",
            "30 encoder_6.block.0.weight\n",
            "31 encoder_6.block.1.weight\n",
            "32 encoder_6.block.1.bias\n",
            "33 encoder_6.block.3.weight\n",
            "34 encoder_6.block.4.weight\n",
            "35 encoder_6.block.4.bias\n",
            "36 decoder_1.block.0.weight\n",
            "37 decoder_1.block.0.bias\n",
            "38 conv1.block.0.weight\n",
            "39 conv1.block.1.weight\n",
            "40 conv1.block.1.bias\n",
            "41 conv1.block.3.weight\n",
            "42 conv1.block.4.weight\n",
            "43 conv1.block.4.bias\n",
            "44 decoder_2.block.0.weight\n",
            "45 decoder_2.block.0.bias\n",
            "46 conv2.block.0.weight\n",
            "47 conv2.block.1.weight\n",
            "48 conv2.block.1.bias\n",
            "49 conv2.block.3.weight\n",
            "50 conv2.block.4.weight\n",
            "51 conv2.block.4.bias\n",
            "52 decoder_3.block.0.weight\n",
            "53 decoder_3.block.0.bias\n",
            "54 conv3.block.0.weight\n",
            "55 conv3.block.1.weight\n",
            "56 conv3.block.1.bias\n",
            "57 conv3.block.3.weight\n",
            "58 conv3.block.4.weight\n",
            "59 conv3.block.4.bias\n",
            "60 decoder_4.block.0.weight\n",
            "61 decoder_4.block.0.bias\n",
            "62 conv4.block.0.weight\n",
            "63 conv4.block.1.weight\n",
            "64 conv4.block.1.bias\n",
            "65 conv4.block.3.weight\n",
            "66 conv4.block.4.weight\n",
            "67 conv4.block.4.bias\n",
            "68 decoder_5.block.0.weight\n",
            "69 decoder_5.block.0.bias\n",
            "70 conv5.block.0.weight\n",
            "71 conv5.block.1.weight\n",
            "72 conv5.block.1.bias\n",
            "73 conv5.block.3.weight\n",
            "74 conv5.block.4.weight\n",
            "75 conv5.block.4.bias\n",
            "76 classifier.weight\n",
            "77 classifier.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zCc-jAO50iKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model parameters\n",
        "params_dir = \"/content/gdrive/MyDrive/GEOG315/A5_resources/trained_model_parameters/trained_unet_final_state.pth\"\n",
        "\n",
        "model = load_params(params_dir, \n",
        "                    model, \n",
        "                    freeze_params=list(range(36)))"
      ],
      "metadata": {
        "id": "p-tmirARyUm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochIterater(train_loader, \n",
        "              val_loader, \n",
        "              model, \n",
        "              criterion, \n",
        "              WorkingFolder, \n",
        "              initial_lr, \n",
        "              epochs)"
      ],
      "metadata": {
        "id": "-wgSqUbhMOGN",
        "outputId": "ecbda628-3ecb-4a1b-e587-86713ded8013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------GPU available----------\n",
            "Epoch [1/25]\n",
            "Training loss: 0.2970\n",
            "validation loss: 0.33241550868651903\n",
            "LR: [0.01]\n",
            "Epoch [2/25]\n",
            "Training loss: 0.2901\n",
            "validation loss: 0.32835977835977664\n",
            "LR: [0.01]\n",
            "Epoch [3/25]\n",
            "Training loss: 0.2714\n",
            "validation loss: 0.3516396385480146\n",
            "LR: [0.009000000000000001]\n",
            "Epoch [4/25]\n",
            "Training loss: 0.2469\n",
            "validation loss: 0.1937505190306511\n",
            "LR: [0.009000000000000001]\n",
            "Epoch [5/25]\n",
            "Training loss: 0.2310\n",
            "validation loss: 0.37982539221888684\n",
            "LR: [0.009000000000000001]\n",
            "Epoch [6/25]\n",
            "Training loss: 0.2498\n",
            "validation loss: 0.19529496359287715\n",
            "LR: [0.008100000000000001]\n",
            "Epoch [7/25]\n",
            "Training loss: 0.2227\n",
            "validation loss: 0.182307988466298\n",
            "LR: [0.008100000000000001]\n",
            "Epoch [8/25]\n",
            "Training loss: 0.2197\n",
            "validation loss: 0.22477291518303213\n",
            "LR: [0.008100000000000001]\n",
            "Epoch [9/25]\n",
            "Training loss: 0.2064\n",
            "validation loss: 0.265312597888415\n",
            "LR: [0.007290000000000001]\n",
            "Epoch [10/25]\n",
            "Training loss: 0.2031\n",
            "validation loss: 0.19747618530861666\n",
            "LR: [0.007290000000000001]\n",
            "Epoch [11/25]\n",
            "Training loss: 0.2028\n",
            "validation loss: 0.3228435395438163\n",
            "LR: [0.007290000000000001]\n",
            "Epoch [12/25]\n",
            "Training loss: 0.2018\n",
            "validation loss: 0.50329939459191\n",
            "LR: [0.006561000000000002]\n",
            "Epoch [13/25]\n",
            "Training loss: 0.2093\n",
            "validation loss: 0.22152676408896682\n",
            "LR: [0.006561000000000002]\n",
            "Epoch [14/25]\n",
            "Training loss: 0.2019\n",
            "validation loss: 0.21751732605158305\n",
            "LR: [0.006561000000000002]\n",
            "Epoch [15/25]\n",
            "Training loss: 0.1827\n",
            "validation loss: 0.1943830999805302\n",
            "LR: [0.005904900000000002]\n",
            "Epoch [16/25]\n",
            "Training loss: 0.1996\n",
            "validation loss: 0.23963935093068686\n",
            "LR: [0.005904900000000002]\n",
            "Epoch [17/25]\n",
            "Training loss: 0.2108\n",
            "validation loss: 0.18774763195485364\n",
            "LR: [0.005904900000000002]\n",
            "Epoch [18/25]\n",
            "Training loss: 0.1939\n",
            "validation loss: 0.1949180014675758\n",
            "LR: [0.005314410000000002]\n",
            "Epoch [19/25]\n",
            "Training loss: 0.1901\n",
            "validation loss: 0.2578335774482274\n",
            "LR: [0.005314410000000002]\n",
            "Epoch [20/25]\n",
            "Training loss: 0.1828\n",
            "validation loss: 0.2267118209331739\n",
            "LR: [0.005314410000000002]\n",
            "Epoch [21/25]\n",
            "Training loss: 0.1913\n",
            "validation loss: 0.17488940530380265\n",
            "LR: [0.004782969000000002]\n",
            "Epoch [22/25]\n",
            "Training loss: 0.1788\n",
            "validation loss: 0.2038593414987697\n",
            "LR: [0.004782969000000002]\n",
            "Epoch [23/25]\n",
            "Training loss: 0.1866\n",
            "validation loss: 0.1845894211628398\n",
            "LR: [0.004782969000000002]\n",
            "Epoch [24/25]\n",
            "Training loss: 0.1851\n",
            "validation loss: 0.19647951637868022\n",
            "LR: [0.004304672100000002]\n",
            "Epoch [25/25]\n",
            "Training loss: 0.1881\n",
            "validation loss: 0.23960096560052183\n",
            "LR: [0.004304672100000002]\n",
            "--------------- Training finished in 0:00:21 ---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \n",
        "           os.path.join(Path(WorkingFolder), \"trained_unet_final_state_tuned.pth\"))"
      ],
      "metadata": {
        "id": "kRDsEwwpPX6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Doing prediction (Inference)**"
      ],
      "metadata": {
        "id": "aPXjB5cazAq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/GEOG315/A5_resources/prediction_scenes\"\n",
        "csv_name = \"pond_scenes_inference.csv\"\n",
        "patch_size = 256\n",
        "overlap = 28\n",
        "save_dir = \"/content/gdrive/MyDrive/GEOG315/Lab5/test_a5\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "metadata": {
        "id": "KkBWYiIfXN-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are running the prediction at a different time with a new session\n",
        "# then you need to initialize the model and load the parameters. \n",
        "# OTHERWISE IGNORE THIS CELL\n",
        "params_dir_epoch50 = \"/content/gdrive/MyDrive/GEOG315/Lab5/test_a5/trained_unet_final_state.pth\"\n",
        "params_dir_sam = \"/content/gdrive/MyDrive/GEOG315/A5_resources/trained_model_parameters/trained_unet_final_state.pth\"\n",
        "params_dir_tuned = \"/content/gdrive/MyDrive/GEOG315/Lab5/test_a5_fineT/trained_unet_final_state_tuned.pth\"\n",
        "\n",
        "model = Unet(n_classes, in_channels, filter_config, dropout_rate)\n",
        "\n",
        "model = load_params(params_dir_tuned, model, freeze_params=None)"
      ],
      "metadata": {
        "id": "41v06c76XYq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_pred(usage, csv_name, patch_size, overlap, catalog_row):\n",
        "    pred_dataset = AquacultureData(src_dir,\n",
        "                                   usage = usage,\n",
        "                                   apply_normalization=False,\n",
        "                                   csv_name = csv_name,\n",
        "                                   patch_size = patch_size,\n",
        "                                   overlap = overlap,\n",
        "                                   catalog_index=catalog_row)\n",
        "    \n",
        "    data_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False)\n",
        "    meta = pred_dataset.meta\n",
        "    tile = pred_dataset.tile\n",
        "    \n",
        "    return data_loader, meta, tile"
      ],
      "metadata": {
        "id": "AiGxCr_UzCYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tile_count = len(pd.read_csv(os.path.join(src_dir, csv_name)))"
      ],
      "metadata": {
        "id": "qcUEFtV_zLpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(tile_count):\n",
        "    pred_data = load_data_pred(\"inference\", csv_name, patch_size, overlap, i)\n",
        "    do_prediction(pred_data, model, overlap, device, save_dir)"
      ],
      "metadata": {
        "id": "2IAcfG34zM0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e678adb-a2d8-467d-c7f4-be152fed72ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches: 88\n",
            "Patched from:\n",
            "[[128, 128], [328, 128], [528, 128], [728, 128], [928, 128], [1128, 128], [1328, 128], [1528, 128], [1728, 128], [128, 328], [328, 328], [528, 328], [728, 328], [928, 328], [1128, 328], [1328, 328], [1528, 328], [1728, 328], [128, 528], [328, 528], [528, 528], [728, 528], [928, 528], [1128, 528], [1328, 528], [1528, 528], [1728, 528], [128, 728], [328, 728], [528, 728], [728, 728], [928, 728], [1128, 728], [1328, 728], [1528, 728], [1728, 728], [128, 928], [328, 928], [528, 928], [728, 928], [928, 928], [1128, 928], [1328, 928], [1528, 928], [1728, 928], [128, 1128], [328, 1128], [528, 1128], [728, 1128], [928, 1128], [1128, 1128], [1328, 1128], [1528, 1128], [1728, 1128], [128, 1328], [328, 1328], [528, 1328], [728, 1328], [928, 1328], [1128, 1328], [1328, 1328], [1528, 1328], [1728, 1328], [328, 1528], [528, 1528], [728, 1528], [928, 1528], [1128, 1528], [1328, 1528], [1528, 1528], [1728, 1528], [328, 1728], [528, 1728], [728, 1728], [928, 1728], [1128, 1728], [1328, 1728], [1528, 1728], [1728, 1728], [928, 1928], [1128, 1928], [1328, 1928], [1528, 1928], [1728, 1928], [928, 2128], [1128, 2128], [1328, 2128], [1528, 2128]]\n",
            "--------------88 patches cropped--------------\n",
            "--------------------- Start Inference(Test) ---------------------\n",
            "---------------- Inference finished in 0:00:02 seconds ----------------\n",
            "Number of patches: 356\n",
            "Patched from:\n",
            "[[128, 128], [328, 128], [528, 128], [728, 128], [928, 128], [1128, 128], [1328, 128], [1528, 128], [1728, 128], [1928, 128], [2128, 128], [2328, 128], [2528, 128], [2728, 128], [2928, 128], [3128, 128], [3328, 128], [3528, 128], [3728, 128], [128, 328], [328, 328], [528, 328], [728, 328], [928, 328], [1128, 328], [1328, 328], [1528, 328], [1728, 328], [1928, 328], [2128, 328], [2328, 328], [2528, 328], [2728, 328], [2928, 328], [3128, 328], [3328, 328], [3528, 328], [3728, 328], [128, 528], [328, 528], [528, 528], [728, 528], [928, 528], [1128, 528], [1328, 528], [1528, 528], [1728, 528], [1928, 528], [2128, 528], [2328, 528], [2528, 528], [2728, 528], [2928, 528], [3128, 528], [3328, 528], [3528, 528], [3728, 528], [128, 728], [328, 728], [528, 728], [728, 728], [928, 728], [1128, 728], [1328, 728], [1528, 728], [1728, 728], [1928, 728], [2128, 728], [2328, 728], [2528, 728], [2728, 728], [2928, 728], [3128, 728], [3328, 728], [3528, 728], [3728, 728], [928, 928], [1128, 928], [1328, 928], [1528, 928], [1728, 928], [1928, 928], [2128, 928], [2328, 928], [2528, 928], [2728, 928], [2928, 928], [3128, 928], [3328, 928], [3528, 928], [3728, 928], [128, 1128], [328, 1128], [528, 1128], [728, 1128], [928, 1128], [1128, 1128], [1328, 1128], [1528, 1128], [1728, 1128], [1928, 1128], [2128, 1128], [2328, 1128], [2528, 1128], [2728, 1128], [2928, 1128], [3128, 1128], [3328, 1128], [3528, 1128], [3728, 1128], [128, 1328], [328, 1328], [528, 1328], [728, 1328], [928, 1328], [1128, 1328], [1328, 1328], [1528, 1328], [1728, 1328], [1928, 1328], [2128, 1328], [2328, 1328], [2528, 1328], [2728, 1328], [2928, 1328], [3128, 1328], [3328, 1328], [3528, 1328], [3728, 1328], [128, 1528], [328, 1528], [528, 1528], [728, 1528], [928, 1528], [1128, 1528], [1328, 1528], [1528, 1528], [1728, 1528], [1928, 1528], [2128, 1528], [2328, 1528], [2528, 1528], [2728, 1528], [2928, 1528], [3128, 1528], [3328, 1528], [3528, 1528], [3728, 1528], [128, 1728], [328, 1728], [528, 1728], [728, 1728], [928, 1728], [1128, 1728], [1328, 1728], [1528, 1728], [1728, 1728], [1928, 1728], [2128, 1728], [2328, 1728], [2528, 1728], [2728, 1728], [2928, 1728], [3128, 1728], [3328, 1728], [3528, 1728], [3728, 1728], [128, 1928], [328, 1928], [528, 1928], [728, 1928], [928, 1928], [1128, 1928], [1328, 1928], [1528, 1928], [1728, 1928], [1928, 1928], [2128, 1928], [2328, 1928], [2528, 1928], [2728, 1928], [2928, 1928], [3128, 1928], [3328, 1928], [3528, 1928], [3728, 1928], [128, 2128], [328, 2128], [528, 2128], [728, 2128], [928, 2128], [1128, 2128], [1328, 2128], [1528, 2128], [1728, 2128], [1928, 2128], [2128, 2128], [2328, 2128], [2528, 2128], [2728, 2128], [2928, 2128], [3128, 2128], [3328, 2128], [3528, 2128], [3728, 2128], [128, 2328], [328, 2328], [528, 2328], [728, 2328], [928, 2328], [1128, 2328], [1328, 2328], [1528, 2328], [1728, 2328], [1928, 2328], [2128, 2328], [2328, 2328], [2528, 2328], [2728, 2328], [2928, 2328], [3128, 2328], [3328, 2328], [3528, 2328], [128, 2528], [328, 2528], [528, 2528], [728, 2528], [928, 2528], [1128, 2528], [1328, 2528], [1528, 2528], [1728, 2528], [1928, 2528], [2128, 2528], [2328, 2528], [2528, 2528], [2728, 2528], [2928, 2528], [3128, 2528], [3328, 2528], [3528, 2528], [3728, 2528], [128, 2728], [328, 2728], [528, 2728], [728, 2728], [928, 2728], [1128, 2728], [1328, 2728], [1528, 2728], [1728, 2728], [1928, 2728], [2128, 2728], [2328, 2728], [2528, 2728], [2728, 2728], [2928, 2728], [3128, 2728], [3328, 2728], [3528, 2728], [3728, 2728], [128, 2928], [328, 2928], [528, 2928], [728, 2928], [928, 2928], [1128, 2928], [1328, 2928], [1528, 2928], [1728, 2928], [1928, 2928], [2128, 2928], [2328, 2928], [2528, 2928], [2728, 2928], [2928, 2928], [3128, 2928], [3328, 2928], [3528, 2928], [3728, 2928], [128, 3128], [328, 3128], [528, 3128], [728, 3128], [928, 3128], [1128, 3128], [1328, 3128], [1528, 3128], [1728, 3128], [1928, 3128], [2128, 3128], [2328, 3128], [2528, 3128], [2728, 3128], [2928, 3128], [3128, 3128], [3328, 3128], [3528, 3128], [3728, 3128], [128, 3328], [328, 3328], [528, 3328], [728, 3328], [928, 3328], [1128, 3328], [1328, 3328], [1528, 3328], [1728, 3328], [1928, 3328], [2128, 3328], [2328, 3328], [2528, 3328], [2728, 3328], [2928, 3328], [3128, 3328], [3328, 3328], [3528, 3328], [3728, 3328], [128, 3528], [328, 3528], [528, 3528], [728, 3528], [928, 3528], [1128, 3528], [1328, 3528], [1528, 3528], [1728, 3528], [1928, 3528], [2128, 3528], [2328, 3528], [2528, 3528], [2728, 3528], [2928, 3528], [3128, 3528], [3328, 3528], [3528, 3528], [3728, 3528], [128, 3728], [328, 3728], [528, 3728], [728, 3728], [928, 3728], [1128, 3728], [1328, 3728], [1528, 3728], [1728, 3728], [1928, 3728], [2128, 3728], [2328, 3728], [2528, 3728], [2728, 3728], [2928, 3728], [3128, 3728], [3328, 3728], [3528, 3728], [3728, 3728]]\n",
            "--------------356 patches cropped--------------\n",
            "--------------------- Start Inference(Test) ---------------------\n",
            "---------------- Inference finished in 0:00:08 seconds ----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 25/25 points on CA2</font>**"
      ],
      "metadata": {
        "id": "L805Tqk_xJvO"
      }
    }
  ]
}